{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2888fe66-a41f-4faf-a84b-fd0401818bac",
   "metadata": {},
   "source": [
    "Data Science Project: Phage Lysin Sequence Alignment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ef5199-9ad9-4822-a751-a7a704dc5d1b",
   "metadata": {},
   "source": [
    "With the ongoing rise of antibiotic resistance in bacteria, including challenging pathogens like Mycobacteria, finding alternative treatments for patients is becoming increasingly urgent. An emerging approach is the use of phage lysins to effectively target and eradicate specific bacterial infections. Lysins are enzymes produced by bacteriophages, they possess domains that adhere to bacterial cell wall components and trigger hydrolysis, ultimately resulting in the elimination of the host bacterium. Understanding the conservation and variability of amino acid sequences, especially in the context of Mycobacteria infections, is crucial for developing effective therapeutic lysin treatments. In this project, we will identify conserved regions, assess sequence similarity, and uncover evolutionary patterns to inform the development of targeted antibacterial therapies, specifically focusing on Mycobacteria skin and lung infections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af999b0a-737a-4143-83b2-d67743ebfe1b",
   "metadata": {},
   "source": [
    "Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "782ab4a7-ce25-4c3a-8cb0-7301b038ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import requests\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4762159-d555-4256-a459-b9dbd326437f",
   "metadata": {},
   "source": [
    "Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2030275f-d393-49b1-8ca6-1f2c4e53ecbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 10000 records (Chunk 1)\n",
      "Fetched 10000 records (Chunk 2)\n",
      "Fetched 10000 records (Chunk 3)\n",
      "Fetched 10000 records (Chunk 4)\n",
      "Fetched 10000 records (Chunk 5)\n",
      "Fetched 10000 records (Chunk 6)\n",
      "Fetched 10000 records (Chunk 7)\n",
      "Fetched 10000 records (Chunk 8)\n",
      "Fetched 10000 records (Chunk 9)\n",
      "Fetched 10000 records (Chunk 10)\n",
      "Fetched 10000 records (Chunk 11)\n",
      "Fetched 10000 records (Chunk 12)\n",
      "Fetched 10000 records (Chunk 13)\n",
      "Fetched 10000 records (Chunk 14)\n",
      "Fetched 10000 records (Chunk 15)\n",
      "Fetched 10000 records (Chunk 16)\n",
      "Fetched 10000 records (Chunk 17)\n",
      "Fetched 10000 records (Chunk 18)\n",
      "Fetched 10000 records (Chunk 19)\n",
      "Fetched 10000 records (Chunk 20)\n",
      "Fetched 10000 records (Chunk 21)\n",
      "Fetched 10000 records (Chunk 22)\n",
      "Fetched 10000 records (Chunk 23)\n",
      "Fetched 10000 records (Chunk 24)\n",
      "Fetched 10000 records (Chunk 25)\n",
      "Fetched 10000 records (Chunk 26)\n",
      "Fetched 10000 records (Chunk 27)\n",
      "Fetched 10000 records (Chunk 28)\n",
      "Fetched 10000 records (Chunk 29)\n",
      "Fetched 10000 records (Chunk 30)\n",
      "Fetched 10000 records (Chunk 31)\n",
      "Fetched 10000 records (Chunk 32)\n",
      "Fetched 10000 records (Chunk 33)\n",
      "Fetched 10000 records (Chunk 34)\n",
      "Fetched 10000 records (Chunk 35)\n",
      "Fetched 10000 records (Chunk 36)\n",
      "Fetched 10000 records (Chunk 37)\n",
      "Fetched 10000 records (Chunk 38)\n",
      "Fetched 10000 records (Chunk 39)\n",
      "Fetched 10000 records (Chunk 40)\n",
      "Fetched 10000 records (Chunk 41)\n",
      "Fetched 10000 records (Chunk 42)\n",
      "Fetched 10000 records (Chunk 43)\n",
      "Fetched 10000 records (Chunk 44)\n",
      "Fetched 10000 records (Chunk 45)\n",
      "Fetched 10000 records (Chunk 46)\n",
      "Fetched 10000 records (Chunk 47)\n",
      "Fetched 10000 records (Chunk 48)\n",
      "Fetched 1205 records (Chunk 49)\n",
      "Data fetched and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Fetch genes data from phagesdb API\n",
    "\n",
    "# URL of the API endpoint\n",
    "url = \"https://phagesdb.org/api/genes/\"\n",
    "\n",
    "# Define the chunk size\n",
    "chunk_size = 10000  # You can adjust this value based on your needs\n",
    "\n",
    "# Initialize variables for pagination\n",
    "page = 1\n",
    "total_records = None\n",
    "\n",
    "# Open the JSON file in write mode\n",
    "with open('phagesdb_genes.json', 'w') as file:\n",
    "    # Write an opening bracket to indicate the start of a JSON array\n",
    "    file.write(\"[\\n\")\n",
    "   \n",
    "    # Fetch data in chunks until all records are retrieved\n",
    "    while total_records is None or (page - 1) * chunk_size < total_records:\n",
    "        # Make a request to the API with pagination parameters\n",
    "        response = requests.get(url, params={'page': page, 'page_size': chunk_size})\n",
    "       \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Convert the response to JSON format\n",
    "            data = response.json()\n",
    "           \n",
    "            # Update the total number of records if it's not set yet\n",
    "            if total_records is None:\n",
    "                total_records = data['count']\n",
    "           \n",
    "            # Write the fetched data from the current page to the JSON file\n",
    "            json.dump(data['results'], file)\n",
    "           \n",
    "            # Write a comma after each chunk except for the last one\n",
    "            if (page - 1) * chunk_size + len(data['results']) < total_records:\n",
    "                file.write(\",\\n\")\n",
    "           \n",
    "            # Print a message to indicate progress\n",
    "            print(f\"Fetched {len(data['results'])} records (Chunk {page})\")\n",
    "        else:\n",
    "            # Print an error message if the request was not successful\n",
    "            print(f\"Error fetching data: {response.status_code}\")\n",
    "            break\n",
    "       \n",
    "        # Increment the page number for the next request\n",
    "        page += 1\n",
    "   \n",
    "    # Write a closing bracket to indicate the end of the JSON array\n",
    "    file.write(\"\\n]\\n\")\n",
    "\n",
    "# Print a message to indicate successful completion\n",
    "print(\"Data fetched and saved successfully!\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92294059-0477-4abc-83c5-522a9a1c46cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lists in the final JSON data: 481205\n",
      "Combined JSON data saved to combined_data_output.json\n"
     ]
    }
   ],
   "source": [
    "#Combine outer lists generated from chunk\n",
    "\n",
    "# Read the JSON file\n",
    "with open('phagesdb_genes.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Combine the outer lists\n",
    "combined_data = sum(data, [])\n",
    "\n",
    "# Determine the number of lists in the final JSON data\n",
    "num_lists = len(combined_data)\n",
    "print(\"Number of lists in the final JSON data:\", num_lists)\n",
    "\n",
    "# Define the file path for the new JSON file\n",
    "output_file = 'combined_data_output.json'\n",
    "\n",
    "# Write the combined JSON data to the new file\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(combined_data, file)\n",
    "\n",
    "print(\"Combined JSON data saved to\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4daf15-9224-4b81-bf3e-ab8299e8f3d6",
   "metadata": {},
   "source": [
    "Data Definition and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98207900-e014-4688-bfde-4b010bddc4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dictionaries: 962410\n",
      "Number of lists: 481206\n",
      "Total number of elements: 6736870\n"
     ]
    }
   ],
   "source": [
    "# Count number of dictionaries, lists, and elements in json file\n",
    "\n",
    "# Read the JSON file\n",
    "with open('combined_data_output.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize counters for dictionaries, lists, and total elements\n",
    "num_dicts = 0\n",
    "num_lists = 0\n",
    "num_elements = 0\n",
    "\n",
    "# Function to recursively count elements\n",
    "def count_elements(data):\n",
    "    global num_dicts, num_lists, num_elements\n",
    "    if isinstance(data, dict):\n",
    "        num_dicts += 1\n",
    "        for value in data.values():\n",
    "            count_elements(value)\n",
    "    elif isinstance(data, list):\n",
    "        num_lists += 1\n",
    "        for item in data:\n",
    "            count_elements(item)\n",
    "    else:\n",
    "        num_elements += 1\n",
    "\n",
    "# Call the function to count elements\n",
    "count_elements(data)\n",
    "\n",
    "print(\"Number of dictionaries:\", num_dicts)\n",
    "print(\"Number of lists:\", num_lists)\n",
    "print(\"Total number of elements:\", num_elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59dbc21c-d120-4fe0-aa24-71a5d16f5e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the first dictionary:\n",
      "GeneID\n",
      "PhageID\n",
      "phams\n",
      "Start\n",
      "Stop\n",
      "Length\n",
      "Name\n",
      "translation\n",
      "Orientation\n",
      "Notes\n"
     ]
    }
   ],
   "source": [
    "# Get keys of first dictionary in json\n",
    "\n",
    "# Analyze the first dictionary\n",
    "analyze_first_dict(data)\n",
    "\n",
    "# Access the first dictionary (assuming the JSON data is a list of dictionaries)\n",
    "first_dict = data[0]\n",
    "\n",
    "# Get the keys of the first dictionary\n",
    "keys = first_dict.keys()\n",
    "\n",
    "# Print the keys\n",
    "print(\"Keys in the first dictionary:\")\n",
    "for key in keys:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4b88fd7-a3ca-4fff-9fab-7a65ee889198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values found.\n"
     ]
    }
   ],
   "source": [
    "# Check for Missing Values in json\n",
    "\n",
    "missing_values = {}\n",
    "for idx, item in enumerate(data):\n",
    "    for key, value in item.items():\n",
    "        if value is None:\n",
    "            if key not in missing_values:\n",
    "                missing_values[key] = []\n",
    "            missing_values[key].append(idx)\n",
    "\n",
    "if missing_values:\n",
    "    print(\"Missing values found:\")\n",
    "    for key, indices in missing_values.items():\n",
    "        print(f\"Key: {key}, Missing in indices: {indices}\")\n",
    "else:\n",
    "    print(\"No missing values found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5ed295f-02d8-4a07-a0c0-2368d0140c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering dictionaries...\n",
      "Filtering complete.\n",
      "Saving filtered dictionaries...\n",
      "Filtered dictionaries saved to 'lysin_dictionaries.json'\n",
      "Number of dictionaries saved: 7191\n"
     ]
    }
   ],
   "source": [
    "# Filter and save any dictionaries that contain Lysin\n",
    "\n",
    "# Function to check if any form of \"lysin\" is present in the Notes\n",
    "def contains_lysin(notes):\n",
    "    lysin_pattern = re.compile(r'lysin', re.IGNORECASE)\n",
    "    return bool(lysin_pattern.search(notes))\n",
    "\n",
    "# Filter for dictionaries where the Notes key contains any form of \"lysin\"\n",
    "lysin_dictionaries = []\n",
    "\n",
    "print(\"Filtering dictionaries...\")\n",
    "\n",
    "for item in data:\n",
    "    notes = item.get('Notes', '')\n",
    "    if contains_lysin(notes):\n",
    "        lysin_dictionaries.append(item)\n",
    "\n",
    "print(\"Filtering complete.\")\n",
    "\n",
    "# Specify the path to the new JSON file\n",
    "output_file_path = 'lysin_dictionaries.json'\n",
    "\n",
    "# Write the filtered dictionaries to the new JSON file\n",
    "print(\"Saving filtered dictionaries...\")\n",
    "\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    json.dump(lysin_dictionaries, output_file)\n",
    "\n",
    "num_saved_dictionaries = len(lysin_dictionaries)\n",
    "print(f\"Filtered dictionaries saved to '{output_file_path}'\")\n",
    "print(f\"Number of dictionaries saved: {num_saved_dictionaries}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da3f54c8-47f7-448d-a8e0-e39ca2bb3b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dictionaries: 14382\n",
      "Number of lists: 7192\n",
      "Total number of elements: 100674\n"
     ]
    }
   ],
   "source": [
    "# Count dictionaries, lists, and elements in filtered json\n",
    "\n",
    "with open('lysin_dictionaries.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize counters for dictionaries, lists, and total elements\n",
    "num_dicts = 0\n",
    "num_lists = 0\n",
    "num_elements = 0\n",
    "\n",
    "# Function to recursively count elements\n",
    "def count_elements(data):\n",
    "    global num_dicts, num_lists, num_elements\n",
    "    if isinstance(data, dict):\n",
    "        num_dicts += 1\n",
    "        for value in data.values():\n",
    "            count_elements(value)\n",
    "    elif isinstance(data, list):\n",
    "        num_lists += 1\n",
    "        for item in data:\n",
    "            count_elements(item)\n",
    "    else:\n",
    "        num_elements += 1\n",
    "\n",
    "# Call the function to count elements\n",
    "count_elements(data)\n",
    "\n",
    "print(\"Number of dictionaries:\", num_dicts)\n",
    "print(\"Number of lists:\", num_lists)\n",
    "print(\"Total number of elements:\", num_elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d6792d-d8e9-4e5b-b614-35e4ed0edeb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
